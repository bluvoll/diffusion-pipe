# Anima Training Configuration
# Anima is based on Cosmos-Predict2 with dual text encoders (Qwen3-0.6B + T5's Tokenizer)

output_dir = '/home/bluvoll/anima-test'
dataset = 'dataset.toml'

# Training settings
epochs = 50
micro_batch_size_per_gpu = 12
pipeline_stages = 1 #This shards your model across GPUs, without a fused optimizer, or fused optimizer groups, you need 33GB for batch size 1 Full Finetuning
gradient_accumulation_steps = 4
gradient_clipping = 1.0
# Warmup steps = optimizer steps (after accumulation), not samples
# Total warmup samples = warmup_steps × micro_batch_size × accumulation_steps
warmup_steps = 10
train_llm_adapter = false #this is like training Clip in SDXL, not the exact same but similar set to true if you know 

# Save settings
save_every_n_epochs = 5
#checkpoint_every_n_minutes = 120
activation_checkpointing = 'unsloth'
save_dtype = 'bfloat16'


# Block swapping for lower VRAM usage (optional)
# blocks_to_swap = 10

[model]
type = 'anima'

# Anima transformer checkpoint (contains both MiniTrainDIT and LLMAdapter weights)
transformer_path = '/home/bluvoll/diffusion-pipe/anima/split_files/diffusion_models/anima-preview.safetensors'

# VAE - Qwen Image VAE (16 channel latent space)
# Use the VAE from Qwen-Image model
vae_path = '/home/bluvoll/diffusion-pipe/anima/split_files/vae/qwen_image_vae.safetensors'

# Qwen3-0.6B model path (HuggingFace format directory)
# This should contain the full Qwen3-0.6B model files (config.json, model.safetensors, tokenizer files, etc.)
qwen_path = '/home/bluvoll/diffusion-pipe/qwen0.6/'
# Optional: separate state dict path if not loading from the HF directory
# qwen_state_dict_path = '/path/to/qwen_weights.safetensors'

# Base dtype for all models
dtype = 'bfloat16'

# Transformer dtype (can use fp8 for lower VRAM)
# transformer_dtype = 'float8'

# Timestep sampling method
timestep_sample_method = 'logit_normal'
# Sigmoid scale for logit_normal sampling
sigmoid_scale = 1.0

# Optional: shift parameter for timestep sampling
shift = 3.0

# Text embedding caching (recommended for training efficiency)
# Set to false to enable training-time tag shuffling
cache_text_embeddings = false
shuffle_tags = true #Shuffles tags and tags only.
# Drop 10% of tags randomly (minimum 3 tags always survive)
tag_dropout_percent = 0.10
# Train 10% of samples with empty caption (for CFG/unconditional generation)
caption_dropout_percent = 0.1
caption_mode = "mixed"
# Mixed mode weights (normalized automatically, don't need to sum to 100):
#   tags    = tags only ("1girl, blue hair, smile")
#   nl      = NL caption only ("A girl with blue hair smiles warmly.")
#   tags_nl = tags then NL ("1girl, blue hair. A girl with blue hair smiles.")
#   nl_tags = NL then tags ("A girl with blue hair smiles. 1girl, blue hair")
# If image has no {name}_nl.txt file, falls back to tags-only for that sample
mixed_weights = { tags = 50, nl = 10, tags_nl = 20, nl_tags = 20 } #These are percentages in mixed, so 50% pure tags, 10% pure NL, 20% tags + NL, 20% NL + Tags.

# NL caption processing
# Shuffle sentences in NL captions (splits on ". ")
nl_shuffle_sentences = true
# Keep first sentence in place when shuffling (usually contains framing/subject info)
# nl_keep_first_sentence = false

# Tag shuffling settings
# NOTE: Requires cache_text_embeddings = false to work!
tag_delimiter = ', '
# Keep first N tags in position when shuffling (useful for trigger words)
# These tags are also protected from dropout
# shuffle_keep_first_n = 1

# Protected tags file - tags listed here are never dropped by tag_dropout
# protected_tags_file = '/path/to/protected_tags.txt'

# Debug caption processing
debug_caption_processing = false
# Debug output interval: 0=every sample, N=every N samples, -1=first 10 only
# debug_caption_interval = 100

caching_batch_size = 5

# Optional: quantize Qwen model to NF4 for lower VRAM
qwen_nf4 = true

# Optional: use FP8 for Qwen model weights
# qwen_fp8 = true

# LoRA adapter configuration, with all of this commented this becomes FULL FINETUNING, now works with sharding across gpus.
[adapter]
type = 'lora'
rank = 32
dtype = 'bfloat16'
# Optional: initialize from existing LoRA
# init_from_existing = '/path/to/existing_lora'

[optimizer]
type = 'AdamW8bitKahan'
lr = 3e-5
betas = [0.9, 0.99]
weight_decay = 0.01
eps = 1e-8
# [optimizer] #Original diffusion-pipe optimizer 
# type = 'adamw_optimi'
# lr = 2e-5
# betas = [0.9, 0.99]
# weight_decay = 0.01
# eps = 1e-8
# Alternative: 8-bit AdamW for lower memory
# [optimizer]
# type = 'AdamW8bitKahan'
# lr = 2e-5
# betas = [0.9, 0.99]
# weight_decay = 0.01
# stabilize = false

[monitoring]
enable_wandb = true
wandb_api_key = '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'
wandb_tracker_name = 'Anima-test'
wandb_run_name = 'Kanojos-anima-test-LoRA'
