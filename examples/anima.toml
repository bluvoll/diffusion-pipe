# Example Anima training config
# Anima is based on Cosmos-Predict2 with dual text encoders (Qwen3-0.6B + T5's Tokenizer)

output_dir = '/path/to/training_runs/anima'
dataset = 'path/to/dataset.toml'

# Training settings
epochs = 100
micro_batch_size_per_gpu = 1
pipeline_stages = 1
gradient_accumulation_steps = 1
gradient_clipping = 1.0
# Warmup steps = optimizer steps (after accumulation), not samples
# Total warmup samples = warmup_steps × micro_batch_size × accumulation_steps
warmup_steps = 100
activation_checkpointing = true

# Eval settings
eval_every_n_epochs = 1
eval_before_first_step = true
eval_micro_batch_size_per_gpu = 1
eval_gradient_accumulation_steps = 1

# Save settings
save_every_n_epochs = 5
checkpoint_every_n_minutes = 60
save_dtype = 'bfloat16'

# Model configuration
[model]
type = 'anima'
transformer_path = '/path/to/anima/transformer.safetensors'
vae_path = '/path/to/anima/vae.safetensors'
qwen_path = '/path/to/Qwen3-0.6B'
dtype = 'bfloat16'

# For FP8 transformer (LoRA only)
# transformer_dtype = 'float8'

# Quantize Qwen to NF4 for lower VRAM usage
# qwen_nf4 = true

# ============================================
# Per-Component Learning Rates (Full Finetuning)
# ============================================
# Set different learning rates for different model components.
# If not specified, all components use the optimizer's base lr.
# Set to 0 to freeze that component entirely.
#
# self_attn_lr = 2e-5      # Self-attention layers
# cross_attn_lr = 2e-5     # Cross-attention layers
# mlp_lr = 2e-5            # MLP/feedforward layers
# mod_lr = 2e-5            # AdaLN modulation layers
# llm_adapter_lr = 1e-6    # LLMAdapter (lower LR recommended to preserve text understanding)

# Train LLMAdapter with LoRA (default: false)
# The LLMAdapter bridges Qwen embeddings to the diffusion model.
# Recommended to keep false for most LoRA training to preserve text understanding.
# Only enable if you need the model to learn a specific captioning style.
# train_llm_adapter = false

# Timestep sampling
timestep_sample_method = 'logit_normal'

# IMPORTANT: For caption processing features, set this to false
cache_text_embeddings = false

# ============================================
# Caption Processing Options
# ============================================

# --- Tag Processing ---
# Shuffle tags at training time (requires cache_text_embeddings=false)
shuffle_tags = true
# Tag delimiter (default: ", ")
tag_delimiter = ', '
# Keep first N tags in position when shuffling (useful for trigger words)
# These tags are also protected from dropout
shuffle_keep_first_n = 1

# --- Tag Dropout ---
# Drop a percentage of tags to improve robustness (0.0 to 1.0)
# Minimum 3 tags will always survive
tag_dropout_percent = 0.10
# Path to file with protected tags (one per line, never dropped)
# protected_tags_file = '/path/to/protected_tags.txt'

# --- NL Caption Processing ---
# Enable sentence shuffling for NL captions
nl_shuffle_sentences = false
# Keep first sentence in place when shuffling (usually contains framing info)
nl_keep_first_sentence = false

# --- Full Caption Dropout (CFG Training) ---
# Percentage of samples that train with empty caption (unconditional)
caption_dropout_percent = 0.05

# --- Caption Mode ---
# Mode for combining tags and NL captions:
#   'tags'  - Use tags only (default, backward compatible)
#   'nl'    - Use NL captions only (from {name}_nl.txt files)
#   'mixed' - Weighted random selection between variants
caption_mode = 'tags'

# Weights for mixed mode (normalized automatically, don't need to sum to 100)
# Only used when caption_mode = 'mixed'
#   tags    = tags only ("1girl, blue hair, smile")
#   nl      = NL caption only ("A girl with blue hair smiles warmly.")
#   tags_nl = tags then NL ("1girl, blue hair. A girl with blue hair smiles.")
#   nl_tags = NL then tags ("A girl with blue hair smiles. 1girl, blue hair")
# If image has no {name}_nl.txt file, falls back to tags-only for that sample
# mixed_weights = { tags = 50, nl = 10, tags_nl = 20, nl_tags = 20 }

# --- Debug Options ---
# Print detailed caption processing info
debug_caption_processing = false
# Debug output interval:
#   0 = every sample
#   N = every N samples
#   -1 = first 10 samples only
debug_caption_interval = 100


# ============================================
# LoRA Configuration
# Comment out this section for full finetuning (works with sharding across GPUs)
# ============================================
[adapter]
type = 'lora'
rank = 32
dtype = 'bfloat16'

# ============================================
# Optimizer
# ============================================
[optimizer]
type = 'adamw_optimi'
lr = 2e-5
betas = [0.9, 0.99]
weight_decay = 0.01
eps = 1e-8
# Alternative: 8-bit AdamW for lower memory
# [optimizer]
# type = 'AdamW8bitKahan'
# lr = 2e-5
# betas = [0.9, 0.99]
# weight_decay = 0.01
# stabilize = false


# Alternative: CAME optimizer (Confidence-guided Adaptive Memory Efficient Optimization)
# From pytorch_optimizer - memory-efficient alternative to AdamW with adaptive learning
# Uses 3 betas: (momentum, variance, confidence) for improved stability
# Recommended for large batch training and memory-constrained setups
#
# [optimizer]
# type = 'CAME'
# lr = 2e-4
# betas = [0.9, 0.999, 0.9999]  # (momentum, variance, confidence)
# weight_decay = 0.0
# weight_decouple = true        # Decouple weight decay from gradient update
# fixed_decay = false           # Use fixed weight decay (vs scaled by lr)
# clip_threshold = 1.0          # Gradient clipping threshold
# ams_bound = false             # Use AMSBound variant
# eps1 = 1e-30                  # Epsilon for numerical stability (denominator)
# eps2 = 1e-16                  # Epsilon for numerical stability (confidence)

# ============================================
# Monitoring
# ============================================
[monitoring]
enable_wandb = false
